# let's prune our tree at the 1se complexity level
dengue.tree_prune = prune_1se(dengue.tree)
# this function actually prunes the tree at that level
prune_1se = function(my_tree) {
out = as.data.frame(my_tree$cptable)
thresh = min(out$xerror + out$xstd)
cp_opt = max(out$CP[out$xerror <= thresh])
prune(my_tree, cp=cp_opt)
}
# let's prune our tree at the 1se complexity level
dengue.tree_prune = prune_1se(dengue.tree)
#plot the tree
rpart.plot(dengue.tree_prune, type=4, digits=-5, extra=1, cex=0.5)
# grow a smallish tree
# larger cp and insplit means stop at a smaller tree
dengue.tree = rpart(total_cases~city + season + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train,
control = rpart.control(cp = 0.0001, minsplit = 5))
# plot the tree
# see ?rpart.plot for the various plotting options here (type, extra)
rpart.plot(dengue.tree, digits=-5, type=4, extra=1)
# grow a smallish tree
# larger cp and insplit means stop at a smaller tree
dengue.tree = rpart(total_cases~city + season + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train,
control = rpart.control(cp = 0.0001, minsplit = 10))
# plot the tree
# see ?rpart.plot for the various plotting options here (type, extra)
rpart.plot(dengue.tree, digits=-5, type=4, extra=1)
# cross-validated error plot.
plotcp(dengue.tree)
# you could squint at the table...
printcp(dengue.tree)
# picking the smallest tree whose CV error is within 1 std err of the minimum
cp_1se(dengue.tree)
# let's prune our tree at the 1se complexity level
dengue.tree_prune = prune_1se(dengue.tree)
#plot the tree
rpart.plot(dengue.tree_prune, type=4, digits=-5, extra=1, cex=0.5)
# cross-validated error plot.
plotcp(dengue.tree)
# cross-validated error plot.
# the result is VERY typical of tree models:
# an initial sharp drop followed by a long flat plateau and then a slow rise.
plotcp(load.tree2)
# the vertical bars show the standard error of CV error across the 10 splits
plotcp(load.tree2, ylim=c(0.26, 0.28))
# cross-validated error plot.
# the result is VERY typical of tree models:
# an initial sharp drop followed by a long flat plateau and then a slow rise.
plotcp(load.tree2)
# the vertical bars show the standard error of CV error across the 10 splits
plotcp(load.tree2, ylim=c(0.26, 0.28))
# cross-validated error plot.
plotcp(dengue.tree)
plotcp(load.tree2, ylim=c(0.8, 1.2))
plotcp(load.tree2, ylim=c(0.26, 0.28))
plotcp(dengue.tree, ylim=c(0.26, 0.28))
plotcp(dengue.tree, ylim=c(0.8, 1.2))
# cross-validated error plot.
plotcp(dengue.tree)
#plot the tree
rpart.plot(dengue.tree_prune, type=4, digits=-5, extra=1, cex=0.5)
# cross-validated error plot.
plotcp(dengue.tree_prune)
# you could squint at the table...
printcp(dengue.tree)
# you could squint at the table...
printcp(dengue.tree_prune)
# split into training and testing
train_frac = 0.8
N_train = floor(train_frac*N)
N = nrow(dengue)
# split into training and testing
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
dengue_train = dengue[train_ind,]
dengue_test = dengue[-train_ind,]
# build a classification tree with variables indicated in questions
dengue_tree= rpart(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, data = dengue_train)
# plot the tree
rpart.plot(dengue_tree, type=4, extra=1)
# the various summaries of the tree
print(dengue_tree) # the structure
summary(dengue_tree)  # more detail on the splits
# in-sample fit, i.e. predict on the original training data
# this returns predicted class probabilities
predict(dengue_tree, newdata=dengue_train)
# grow a smallish tree
# larger cp and insplit means stop at a smaller tree
dengue.tree = rpart(total_cases~city + season + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train,
control = rpart.control(cp = 0.0001, minsplit = 10))
# plot the tree
# see ?rpart.plot for the various plotting options here (type, extra)
rpart.plot(dengue.tree, digits=-5, type=4, extra=1)
# cross-validated error plot.
plotcp(dengue.tree)
# you could squint at the table...
printcp(dengue.tree)
# picking the smallest tree whose CV error is within 1 std err of the minimum
cp_1se(dengue.tree)
# let's prune our tree at the 1se complexity level
dengue.tree_prune = prune_1se(dengue.tree)
#plot the tree
rpart.plot(dengue.tree_prune, type=4, digits=-5, extra=1, cex=0.5)
# you could squint at the table...
printcp(dengue.tree)
# in-sample fit, i.e. predict on the original training data
# this returns predicted class probabilities
predict(dengue_tree, newdata=dengue_train)
plotcp(dengue_tree)
printcp(dengue_tree)
cp_1se(dengue_tree)
summary(dengue_tree)  # more detail on the splits
# cross-validated error plot.
plotcp(dengue_tree)
# you could squint at the table...
printcp(dengue_tree)
# picking the smallest tree whose CV error is within 1 std err of the minimum
cp_1se(dengue_tree)
# in-sample fit, i.e. predict on the original training data
# this returns predicted class probabilities
predict(dengue_tree, newdata=dengue_train)
# you could squint at the table...
printcp(dengue.tree)
# cross-validated error plot.
plotcp(dengue.tree)
# grow a smallish tree
# larger cp and insplit means stop at a smaller tree
dengue.tree = rpart(total_cases~city + season + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train,
control = rpart.control(cp = 0.002, minsplit = 10))
# plot the tree
# see ?rpart.plot for the various plotting options here (type, extra)
rpart.plot(dengue.tree, digits=-5, type=4, extra=1)
# cross-validated error plot.
plotcp(dengue.tree)
# you could squint at the table...
printcp(dengue.tree)
# picking the smallest tree whose CV error is within 1 std err of the minimum
cp_1se(dengue.tree)
# grow a smallish tree
# larger cp and insplit means stop at a smaller tree
dengue.tree = rpart(total_cases~city + season + specific_humidity + tdtr_k + precipitation_amt, data=dengue_train,
control = rpart.control(cp = 0.0002, minsplit = 10))
# plot the tree
# see ?rpart.plot for the various plotting options here (type, extra)
rpart.plot(dengue.tree, digits=-5, type=4, extra=1)
# cross-validated error plot.
plotcp(dengue.tree)
# you could squint at the table...
printcp(dengue.tree)
# picking the smallest tree whose CV error is within 1 std err of the minimum
cp_1se(dengue.tree)
# let's prune our tree at the 1se complexity level
dengue.tree_prune = prune_1se(dengue.tree)
#plot the tree
rpart.plot(dengue.tree_prune, type=4, digits=-5, extra=1, cex=0.5)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, mtry = 5, ntree=100)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, mtry = 5, ntree=100)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, data = dengue_train, mtry = 5, ntree=100)
source("C:/Users/Haokun Zhang/Desktop/UTexas classes/Data mining/data mining assignments/data-mining-assignments/ECO 395M Homework 3/ECO HW 3.R")
load_coast = read.csv('../data/load_coast.csv', row.names=1)
library(randomForest)
library(gbm)
load_coast = read.csv('../data/load_coast.csv', row.names=1)
N = nrow(load_coast)
# split into a training and testing set
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
load_train = load_coast[train_ind,]
load_test = load_coast[-train_ind,]
# let's do bagging first
# average over 25 bootstrap samples
# use all candidate variables (mtry=15) in each bootstrapped sample
forest1 = randomForest(COAST ~ ., data = load_train, mtry = 15, ntree=25)
yhat_forest = predict(forest1, load_test)
rmse_forest = mean((yhat_forest - load_test$COAST)^2) %>% sqrt
# now true random forests
# now average over 100 bootstrap samples
# this time only 5 candidate variables (mtry=5) in each bootstrapped sample
forest2 = randomForest(COAST ~ ., data = load_train, mtry = 5, ntree=100)
yhat_forest2 = predict(forest2, load_test)
rmse_forest2 = mean((yhat_forest2 - load_test$COAST)^2) %>% sqrt
boost_ercot = gbm(COAST ~ ., data=load_train,
n.trees=500, shrinkage=.05)
yhat_forest = predict(forest1, load_test)
View(load_coast)
# now true random forests
# now average over 100 bootstrap samples
# this time only 5 candidate variables (mtry=5) in each bootstrapped sample
forest2 = randomForest(COAST ~ ., data = load_train, mtry = 5, ntree=100)
yhat_forest2 = predict(forest2, load_test)
rmse_forest2 = mean((yhat_forest2 - load_test$COAST)^2) %>% sqrt
boost_ercot = gbm(COAST ~ ., data=load_train,
n.trees=500, shrinkage=.05)
yhat_boost = predict(boost_ercot, load_test, n.trees=500)
rmse_boost = mean((yhat_boost - load_test$COAST)^2) %>% sqrt
rmse_boost
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, mtry = 5, ntree=100)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, mtry = 5, ntree=100, na.action = na.roughfix)
?rfImpute
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, na.action = na.roughfix, mtry = 5, ntree=100)
sapply(dengue_train, class)
source("C:/Users/Haokun Zhang/Desktop/UTexas classes/Data mining/data mining assignments/data-mining-assignments/ECO 395M Homework 3/ECO HW 3.R")
source("C:/Users/Haokun Zhang/Desktop/UTexas classes/Data mining/data mining assignments/data-mining-assignments/ECO 395M Homework 3/ECO HW 3.R")
sapply(dengue_train, class)
dengue$total_cases <- as.numeric(dengue$total_cases)
sapply(dengue_train, class)
View(dengue)
dengue$total_cases <- as.numeric(as.character(dengue$total_cases))
View(dengue)
sapply(dengue_train, class)
# split into training and testing
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
dengue_train = dengue[train_ind,]
dengue_test = dengue[-train_ind,]
sapply((dengue_train, class))
sapply((dengue_train, class)
sapply(dengue_train, class)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, na.action = na.roughfix, mtry = 5, ntree=100)
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ . - season - city, data = dengue_train, na.action = na.roughfix, mtry = 5, ntree=100)
dengue$city <- as.factor(dengue$city)
sapply(dengue, class)
View(dengue)
dengue$season <- as.factor(dengue$season)
# split into training and testing
train_frac = 0.8
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
dengue_train = dengue[train_ind,]
dengue_test = dengue[-train_ind,]
# random forests
# average over 100 bootstrap samples
# only 5 candidate variables (mtry=5) in each bootstrapped sample
dengue2 = randomForest(total_cases ~ ., data = dengue_train, na.action = na.roughfix, mtry = 5, ntree=100)
yhat_forest2 = predict(forest2, dengue_test)
yhat_forest2 = predict(dengue2, dengue_test)
yhat_dengue2 = predict(dengue2, dengue_test)
rmse_dengue2 = mean((yhat_dengue2 - dengue_test$total_cases)^2) %>% sqrt
boost_dengue = gbm(total_cases ~ ., data=dengue_train,
n.trees=500, shrinkage=.05)
yhat_boost = predict(boost_dengue, dengue_test, n.trees=500)
rmse_boost = mean((yhat_boost - dengue_test$total_cases)^2) %>% sqrt
rmse_boost
source("C:/Users/Haokun Zhang/Desktop/UTexas classes/Data mining/data mining assignments/data-mining-assignments/ECO 395M Homework 3/ECO HW 3.R")
knitr::opts_chunk$set(echo = TRUE)
cat("The average effect of green certification on the rent income is ", Diff_mean, "\n")
library(tidyverse)
library(tidyverse)
library(ggplot2)
library(rsample)
library(modelr)
library(ggplot2)
library(rsample)
library(modelr)
library(randomForest)
library(ggmap)
library(rpart)       #for fitting decision trees
library(ipred)       #for fitting bagged decision trees
Calihousing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/CAhousing.csv")
# normalize two variables.
Calihousing$normal_Rooms <- ((Calihousing$totalRooms)/(Calihousing$households))
Calihousing$normal_Bedrooms <- ((Calihousing$totalBedrooms)/(Calihousing$households))
# Separate the dataset to spliting and testing data
CAhousing_split = initial_split(Calihousing, prop = 0.8)
CAhousing_training = training(CAhousing_split)
CAhousing_testing = testing(CAhousing_split)
# Separate the dataset to spliting and testing data
CAhousing_split = initial_split(Calihousing, prop = 0.8)
CAhousing_training = training(CAhousing_split)
CAhousing_testing = testing(CAhousing_split)
```{r,echo=FALSE, message= FALSE, warning=FALSE}
#  the Random Forest model
model_random_forest = randomForest(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training)
yhat_test_forest = predict(model_random_forest, CAhousing_testing)
plot(yhat_test_forest, CAhousing_testing$medianHouseValue)
rmse(model_random_forest, CAhousing_testing)
plot(model_random_forest)
#  the bagging model
model_bagging = bagging(formula = medianHouseValue ~ .-totalRooms-totalBedrooms,
data = CAhousing_training,
nbagg=150,coob=T,control = rpart.control(minsplit = 2, cp = 0))
model_CART = rpart(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training)
# the CART model
set.seed(1)
model_CART = rpart(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training)
```{r,echo=FALSE, message= FALSE, warning=FALSE}
# boosting model
set.seed(1)
model_boosting = gbm(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training,
interaction.depth=4, n.trees=400, shrinkage=.05)
model_CART = rpart(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training)
library(gbm)
model_boosting = gbm(medianHouseValue ~ .-totalRooms-totalBedrooms,
data=CAhousing_training,
interaction.depth=4, n.trees=400, shrinkage=.05)
# calculate the rmse of each model
rmse(model_random_forest, CAhousing_testing)
# calculate the rmse of each model
rmse(model_random_forest, CAhousing_testing)
rmse(model_bagging, CAhousing_testing)
rmse(model_CART, CAhousing_testing)
rmse(model_CART, CAhousing_testing)
rmse(model_boosting, CAhousing_testing)
knitr::opts_chunk$set(echo = TRUE)
# calculate the rmse of each model
cat("RMSE of randomForest is ", (rmse(model_random_forest, CAhousing_testing)), "\n")
# calculate the rmse of each model
cat("RMSE is ", rmse(model_random_forest, CAhousing_testing), "\n")
library(tidyverse)
library(tidyverse)
library(ggplot2)
library(rsample)
library(modelr)
library(rsample)
library(modelr)
library(randomForest)
library(rpart)       #for fitting decision trees
library(gbm)
library(ipred)       #for fitting bagged decision trees
library(tidyverse)
library(ggplot2)
library(rsample)
library(modelr)
library(randomForest)
library(ggmap)
library(rpart)       #for fitting decision trees
library(gbm)
library(ipred)       #for fitting bagged decision trees
```{r,echo=FALSE, message= FALSE, warning=FALSE}
Calihousing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/CAhousing.csv")
Calihousing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/CAhousing.csv")
# normalize two variables.
Calihousing$normal_Rooms <- ((Calihousing$totalRooms)/(Calihousing$households))
Calihousing$normal_Bedrooms <- ((Calihousing$totalBedrooms)/(Calihousing$households))
# Separate the dataset to spliting and testing data
CAhousing_split = initial_split(Calihousing, prop = 0.8)
CAhousing_training = training(CAhousing_split)
CAhousing_testing = testing(CAhousing_split)
# calculate the rmse of each model
cat("RMSE is ", rmse(model_random_forest, CAhousing_testing), "\n")
# calculate the rmse of each model
cat("RMSE of RandomForest is ", rmse(model_random_forest, CAhousing_testing), "\n")
cat("RMSE of bagging is ", rmse(model_bagging, CAhousing_testing), "\n")
cat("RMSE of CART is ", rmse(model_CART, CAhousing_testing), "\n")
cat("RMSE of Boosting is ", rmse(model_boosting, CAhousing_testing), "\n")
# plot the picture
# The plot of  original data
qmplot(longitude, latitude, data = Calihousing, color = medianHouseValue,
size = I(2), darken = .2) +
ggtitle("Figure1: Real Median House Value in California") +
xlab("Longitude") + ylab("Latitude") +
scale_colour_gradient(low = "blue", high = "red") +
labs(color = "Median House Value")
# The plot of model's prediction of median House value
yhat = predict(model_bagging, Calihousing)
# The plot of model's prediction of median House value
yhat = predict(model_bagging, Calihousing)
qmplot(longitude, latitude, data = Calihousing, color = yhat, size = I(2), darken = .2) +
xlab("Longitude") +ylab("Latitude") +
ggtitle("Figure2: Predicted CA Median House Value") +
scale_colour_gradient(low = "blue", high = "red") +
labs(color = "Predicted Median House Value")
# The plot of model's errors/residuals
Calihousing$errors = abs(Calihousing$medianHouseValue - yhat)
qmplot(longitude, latitude, data = Calihousing, color = errors, size = I(2), darken = .2) +
xlab("Longitude") + ylab("Latitude") +
ggtitle("Figure3: Residuals of CA Median House Value") +
scale_colour_gradient(low = "blue", high = "red") +
labs(color = "Residuals")
# Question 1 : What causes what?
## Problem 2: How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers’ paper.
They used the district and day fixed effect so they controlled their
analyses in Washington DC. In this situation, the researchers find an
example where they get a lot of police for reasons unrelated to crime to
explore whether there is a causal relationship between more police and
less crime.
They firstly regressed the daily total number of crimes in D.C. on the
high alert level, and secondly regressed the daily total number of
crimes in D.C. on both the high alert level and logged midday METRO
ridership. In the first regression (results in 1st column), the
coefficient -7.316 indicates that daily total number of crimes in D.C.
would decrease by about 7.3 on the high alert days and it is
statistically significant at the 5% level. In the second column (results
in 2nd column), after controlling the ridership, the coefficients
indicate that the coefficient of the high alert level drops to about
6.05, and the number of daily crimes in D.C. would increase by about 1.7
if Metro ridership increases by 10%, and this estimated coefficient is
statistically significant at 1% level. Since this increase is small, we
can learn that the change in ridership is not strongly correlated with
the change in the number of daily crimes in D.C. on high-alert days. We
can conclude that more police results in less crime.
## Problem 3: Why did they have to control for Metro ridership? What was that trying to capture?
Since after adding more cops, they made a hypothesis that the tourists
were less likely to visit Washington or to go out during that particular
time. Then they checked that hypothesis by looking at ridership levels
on the Metro system, and the number of tourists actually was not
diminished on high terror days, so they suggested the number of victims
was largely unchanged. They wanted to capture that the number of
tourists was the same during the high terror days, and then the
reductions of number of crimes was less likely to be related to the
number of tourists.
## Problem 4: Below I am showing you “Table 4” from the researchers’ paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?
The model here describes the effect of high alerts in different
districts on the number of crimes. The regression tells us that the
expected number of daily crimes would decrease by about 2.62 in District
1 (the national mall) during the high alert days, and this estimated
coefficient is statistically significant at a 1% level. The expected
number of daily crimes would decrease by about 0.57 in other districts
but this estimated coefficient is not statistically significant at the
5% level. And in this case, the expected number of daily crimes in D.C.
would increase by about 0.024 if Metro ridership increases by 1%. Since
the coefficient of interaction between high-alert and District 1(the
national mall) is obviously larger than that between high-alert and
other districts, we could believe that the total crime decline during
high-alert periods is mainly concentrated in District 1.
# Question 2: Tree modeling: dengue cases
##   city season total_cases   ndvi_ne   ndvi_nw   ndvi_se   ndvi_sw
## 1   sj spring           4 0.1226000 0.1037250 0.1984833 0.1776167
## 2   sj spring           5 0.1699000 0.1421750 0.1623571 0.1554857
## 3   sj spring           4 0.0322500 0.1729667 0.1572000 0.1708429
## 4   sj spring           3 0.1286333 0.2450667 0.2275571 0.2358857
## 5   sj spring           6 0.1962000 0.2622000 0.2512000 0.2473400
## 6   sj summer           2        NA 0.1748500 0.2543143 0.1817429
##   precipitation_amt air_temp_k avg_temp_k dew_point_temp_k max_air_temp_k
## 1             12.42   297.5729   297.7429         292.4143          299.8
## 2             22.82   298.2114   298.4429         293.9514          300.9
## 3             34.54   298.7814   298.8786         295.4343          300.5
## 4             15.36   298.9871   299.2286         295.3100          301.4
## 5              7.52   299.5186   299.6643         295.8214          301.9
## 6              9.58   299.6300   299.7643         295.8514          302.4
##   min_air_temp_k precip_amt_kg_per_m2 relative_humidity_percent
## 1          295.9                32.00                  73.36571
## 2          296.4                17.94                  77.36857
## 3          297.3                26.10                  82.05286
## 4          297.0                13.90                  80.33714
## 5          297.5                12.20                  80.46000
## 6          298.1                26.49                  79.89143
##   specific_humidity   tdtr_k
## 1          14.01286 2.628571
## 2          15.37286 2.371429
## 3          16.84857 2.300000
## 4          16.67286 2.428571
## 5          17.21000 3.014286
## 6          17.21286 2.100000
##   city season total_cases   ndvi_ne   ndvi_nw   ndvi_se   ndvi_sw
## 1   sj spring           4 0.1226000 0.1037250 0.1984833 0.1776167
## 2   sj spring           5 0.1699000 0.1421750 0.1623571 0.1554857
## 3   sj spring           4 0.0322500 0.1729667 0.1572000 0.1708429
## 4   sj spring           3 0.1286333 0.2450667 0.2275571 0.2358857
## 5   sj spring           6 0.1962000 0.2622000 0.2512000 0.2473400
## 6   sj summer           2 0.1468334 0.1748500 0.2543143 0.1817429
##   precipitation_amt air_temp_k avg_temp_k dew_point_temp_k max_air_temp_k
## 1             12.42   297.5729   297.7429         292.4143          299.8
## 2             22.82   298.2114   298.4429         293.9514          300.9
## 3             34.54   298.7814   298.8786         295.4343          300.5
## 4             15.36   298.9871   299.2286         295.3100          301.4
## 5              7.52   299.5186   299.6643         295.8214          301.9
## 6              9.58   299.6300   299.7643         295.8514          302.4
##   min_air_temp_k precip_amt_kg_per_m2 relative_humidity_percent
## 1          295.9                32.00                  73.36571
## 2          296.4                17.94                  77.36857
## 3          297.3                26.10                  82.05286
## 4          297.0                13.90                  80.33714
## 5          297.5                12.20                  80.46000
## 6          298.1                26.49                  79.89143
##   specific_humidity   tdtr_k
## 1          14.01286 2.628571
## 2          15.37286 2.371429
## 3          16.84857 2.300000
## 4          16.67286 2.428571
## 5          17.21000 3.014286
## 6          17.21286 2.100000
## Distribution not specified, assuming gaussian ...
We predicted the dengue cases with CART model, Random Forest model and
Gradient-boosted model, and find the best model with lowest RMSE.
![](ECO-HW-3-3-_files/figure-markdown_strict/unnamed-chunk-5-1.png)![](ECO-HW-3-3-_files/figure-markdown_strict/unnamed-chunk-5-2.png)![](ECO-HW-3-3-_files/figure-markdown_strict/unnamed-chunk-5-3.png)
